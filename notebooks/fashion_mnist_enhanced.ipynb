{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST Classifier - Enhanced Notebook\n",
    "\n",
    "This notebook demonstrates an improved approach to the Fashion MNIST classification problem with:\n",
    "- Data normalization\n",
    "- Validation split\n",
    "- Training callbacks\n",
    "- Comprehensive visualizations\n",
    "- Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f\"Training images shape: {train_images.shape}\")\n",
    "print(f\"Training labels shape: {train_labels.shape}\")\n",
    "print(f\"Test images shape: {test_images.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"Pixel value range: [{train_images.min()}, {train_images.max()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(train_images[i], cmap='gray')\n",
    "    plt.title(class_names[train_labels[i]], fontsize=10)\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Sample Images from Fashion MNIST Dataset', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(class_names, counts, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Class', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Class Distribution in Training Set', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to [0, 1]\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Add channel dimension for CNN\n",
    "train_images = np.expand_dims(train_images, axis=-1)\n",
    "test_images = np.expand_dims(test_images, axis=-1)\n",
    "\n",
    "print(f\"Normalized training images shape: {train_images.shape}\")\n",
    "print(f\"Pixel value range after normalization: [{train_images.min():.2f}, {train_images.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Improved CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN model with batch normalization and dropout\n",
    "model = tf.keras.Sequential([\n",
    "    # First convolutional block\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    # Second convolutional block\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    # Third convolutional block\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    # Dense layers\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy', \n",
    "             tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3, name='top_3_accuracy')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy, test_top3_acc = model.evaluate(test_images, test_labels, verbose=1)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"Test Top-3 Accuracy: {test_top3_acc:.4f} ({test_top3_acc*100:.2f}%)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "predictions = model.predict(test_images)\n",
    "predicted_labels = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(test_labels, predicted_labels, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualize Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability predictions\n",
    "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "probabilities = probability_model.predict(test_images[:25])\n",
    "\n",
    "# Plot predictions\n",
    "fig, axes = plt.subplots(5, 5, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(25):\n",
    "    ax = axes[i]\n",
    "    img = test_images[i].squeeze()\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    \n",
    "    true_label = test_labels[i]\n",
    "    pred_label = predicted_labels[i]\n",
    "    confidence = probabilities[i][pred_label]\n",
    "    \n",
    "    color = 'green' if true_label == pred_label else 'red'\n",
    "    ax.set_title(f\"True: {class_names[true_label]}\\nPred: {class_names[pred_label]}\\n({confidence:.2%})\",\n",
    "                color=color, fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample Predictions (Green=Correct, Red=Wrong)', \n",
    "            fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Detailed Prediction Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random test image\n",
    "idx = np.random.randint(0, len(test_images))\n",
    "image = test_images[idx]\n",
    "true_label = test_labels[idx]\n",
    "\n",
    "# Get prediction\n",
    "pred_probs = probability_model.predict(np.expand_dims(image, axis=0))[0]\n",
    "pred_label = np.argmax(pred_probs)\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Display image\n",
    "ax1.imshow(image.squeeze(), cmap='gray')\n",
    "color = 'green' if pred_label == true_label else 'red'\n",
    "ax1.set_title(f\"True: {class_names[true_label]}\\nPredicted: {class_names[pred_label]}\\nConfidence: {pred_probs[pred_label]:.2%}\",\n",
    "             color=color, fontsize=12, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Display probability distribution\n",
    "indices = np.arange(len(class_names))\n",
    "colors = ['green' if i == pred_label else 'skyblue' for i in indices]\n",
    "ax2.barh(indices, pred_probs, color=colors)\n",
    "ax2.set_yticks(indices)\n",
    "ax2.set_yticklabels(class_names)\n",
    "ax2.set_xlabel('Probability', fontsize=11)\n",
    "ax2.set_title('Class Probabilities', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('fashion_mnist_cnn_final.h5')\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This enhanced notebook demonstrates:\n",
    "\n",
    "✅ **Data Preprocessing**: Normalization and proper shape handling\n",
    "\n",
    "✅ **Improved Architecture**: CNN with batch normalization and dropout\n",
    "\n",
    "✅ **Training Callbacks**: Early stopping, learning rate reduction, and model checkpointing\n",
    "\n",
    "✅ **Comprehensive Evaluation**: Test accuracy, confusion matrix, and classification report\n",
    "\n",
    "✅ **Rich Visualizations**: Training history, predictions, and probability distributions\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try different architectures (deeper networks, different filter sizes)\n",
    "- Experiment with data augmentation\n",
    "- Use the trained model with the API or Gradio interface\n",
    "- Deploy using Docker containers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
